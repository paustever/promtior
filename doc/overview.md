First, I designed a FastAPI backend and implemented a RAG pipeline using LangChain. I collected data from two main sources: the provided PDF and multiple Promtior website pages. These documents were loaded and split into chunks using a recursive text splitter to optimize retrieval quality. I then generated embeddings and stored them in a Chroma vector database to enable semantic search over the content.

I integrated an OpenAI chat model and built a prompt that forces the assistant to answer only from retrieved context, avoid hallucinations, and keep a professional but friendly tone. I iterated on the prompt to improve answer style and added conversational rules so the assistant responds politely to greetings or thanks instead of incorrectly saying “I don’t know.”

One of the main technical challenges was model and infrastructure compatibility. The original attempt included Ollama/LLaMA2, but deployment constraints on Railway made that complex. I evaluated alternatives and migrated to OpenAI models, which simplified deployment and improved reliability. This required updating embeddings, handling API keys securely through environment variables, and fixing dimension mismatches in the vector database after changing embedding models.

To imporve the experience of the user I decided to add an interface served directly from FastAPI static files, allowing non-technical users to interact with the chatbot instead of only using the API docs endpoint.